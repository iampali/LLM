{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os,sys\n",
    "import ipdb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil\n",
    "import requests, zipfile, io\n",
    "\n",
    "# pytorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# improve performance\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Empty GPU cache memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_url = \"https://ideami.com/llm_train\"\n",
    "# print(\"Wait bro file is downloading\")\n",
    "# response = requests.get(file_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture Parameters\n",
    "\n",
    "batch_size = 8\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "#Hyper Parameters\n",
    "lr = 3e-4\n",
    "dropout = 0.05 # Regularization to prevent overfiting\n",
    "weight_decay = 0.01 #limiting the weight\n",
    "grad_clip = 1.0 # cliping or maximize the gradients or loss to become too large\n",
    "\n",
    "# Training Hyperparameters \n",
    "\n",
    "train_iters = 100000 # epochs\n",
    "eval_interval = 50 # to check if we are not overfiting the data like if the training loss is kept decreasing but the eval loss is not decreasing as the same time that means we're overfiting the data\n",
    "eval_iters = 10 # epochs for evaluation \n",
    "compile = False # To accelarate the GPU performance and reduce memory usage\n",
    "checkpoint_dir = \"new-models/\" # saving the checkout of our models\n",
    "checkpoint_fn = 'latest.pt'\n",
    "checkpoint_load_fn = 'latest.pt'\n",
    "dtype = torch.bfloat16\n",
    "load_pretrained = False\n",
    "\n",
    "# MODE\n",
    "inference = False # bascially evaluation of model after training with new data. right now we'll be training that's why it's false when we complete the training we'll set it to true\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (f\"devie you'll be using {device}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "\n",
    "wandb_log = True\n",
    "wandb_project = \"first_llm\"\n",
    "wandb_run_name = \"llm1\" + datetime.now().strftime(\"%y_%m_%d_%H_%m_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project = wandb_project, name= wandb_run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "\n",
    "with open('wiki.txt', 'r' , encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the trained tokenizer\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file='wiki_tokenizer.model')\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size is {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda x : sp.Encode(x)\n",
    "decode = lambda y : sp.Decode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encode(\"I love you baby\"))\n",
    "print(decode(encode(\"I love you baby\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('encoded_data.pt'):\n",
    "    print('Loading encoding')\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    data = torch.tensor(encode(text),dtype=torch.long)\n",
    "    torch.save(data,'encoded_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9*data_size)\n",
    "train_data = data[:spl]\n",
    "val_data = data[spl:]\n",
    "\n",
    "print(f\"Total Data is {data_size/1e6:.2f} Million | Train Data is {len(train_data)/1e6:.2f} Million | Val Data is {len(val_data)/1e6:.2f} Million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ind = torch.randint(len(data)-context, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context] for i in ind])\n",
    "    y = torch.stack([data[i+1:i+context+1] for i in ind])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size) # eg: 4096 * 384\n",
    "        self.positions = nn.Embedding(context, embed_size) # 512, 384\n",
    "        self.blocks = nn.Sequential(*[Blocks(n_heads) for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input, target=None):\n",
    "        loss = None\n",
    "        BS, SL = input.shape # BS * SL ie 8 * 512\n",
    "        emb = self.embeddings(input) # BS * SL * 384\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL * 384\n",
    "        x = emb + pos # BS * SL* 384\n",
    "        x = self.blocks(x) # BS * SL* 384\n",
    "        x = self.ln(x) # BS * SL* 384\n",
    "        logits = self.final_linear(x) # BS * SL* 4096 ie our vocabulary size\n",
    "        # These logits are the normalized probabilities of every 4096 tokens that can be the next token.\n",
    "        \n",
    "\n",
    "        # Calculating the loss that has between target probabilities and logits as predited probabilities\n",
    "        if target is not None:\n",
    "            BS, SL, VS = logits.shape\n",
    "            logits = logits.view(BS*SL, VS)\n",
    "            target = target.view(BS*SL)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "\n",
    "            # # Manual Calculation\n",
    "            counts = logits.exp()\n",
    "            prob = counts / counts.sum(-1, keepdim = True)\n",
    "            loss2 = prob[torch.arange(BS*SL), target].log().mean()\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    # Generate a new sample\n",
    "\n",
    "    def generate(self, input, max = 500):\n",
    "        for _ in range(max):\n",
    "            input = input[:,-context:] # Here we are taking only last 512 tokens from the input as our llm can only process 512 which is our context lenght at a time.\n",
    "            # input gonna be 1 dimesion tensor of lenght min(len(input), 512)\n",
    "            logits, _ = self(input) # we pass it to the forward function to get logits\n",
    "            logits = logits[:,-1,:] # (1 * 4096) now logits are shape of 8 * 512 * 4096 of normalized probabilities but we need to get the probability of of what will come after last token that's why we just take last dimension of 512\n",
    "            probs = F.softmax(logits, dim=-1) # (1,4096) using softmax function we convert that normalized probability into an actual probability of every toekn present in our vocab size that can come after the last token of our input\n",
    "            next = torch.multinomial(probs, num_samples=1)\n",
    "            # this multinomial function will take 1 num_sample from the entire probabities as a predicted next token\n",
    "            input = torch.cat((input,next), dim=1)\n",
    "            # and we add that next predicted token to the input to got through the loop again keepgin the dim = 1\n",
    "        \n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blocks(nn.Module):\n",
    "    def __init__(self,n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads # just dividing the embed size of every token in 7 different heads, each 7 layers have 7 different heads.\n",
    "        self.ma = Multihead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # This here is called residual network, we did this to prevent vanishing gradient and mapping for increasing computation.\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self,n_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(n_heads * head_size, embed_size, bias=BIAS) # (374 // 7 = 54) and 54 * 7 = 378 so we're going here (378,embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim = -1)\n",
    "        # Each head output will be (BS,SL,head_size)\n",
    "        x = self.combine(x) # this will return (BS,SL,384)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size, 6*embed_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias=BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size,head_size,bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size,head_size,bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size,head_size,bias=BIAS)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context,context)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        BS, SL, VS = x.shape\n",
    "        q = self.queries(x) # BS , SL , 54\n",
    "        k = self.keys(x) # BS , SL , 54\n",
    "        v = self.values(x) # BS , SL , 54\n",
    "\n",
    "        attn_w = q @ k.transpose(-2,-1) * k.shape[-1]**0.5 # BS, SL, SL\n",
    "        # q @ k is matrix multiprication q is BS*512*54 and k is also same that's why we transpose k to make it BS*54*512 so that we can matrix mulitply them\n",
    "        # and k.shape[-1]**0.5 this part is used to normalize the value and keep them between 0 and 1 as softmax function do.\n",
    "        # that part was discovered by scientist for nomalizing values\n",
    "\n",
    "        attr_w = attn_w.masked_fill(self.tril[:SL,:SL] == 0, float('-inf'))\n",
    "        attn_w = F.softmax(attn_w, dim = -1) # BS, SL, SL\n",
    "\n",
    "        x = attn_w @ v # BS, SL, 54 as attn_w (BS, 512, 512) and v (BS, 512*54) so the middle one should be the same tha's why we didn't tranpose the v matrix here\n",
    "\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = get_batch('train')\n",
    "# model = GPT()\n",
    "# model = model.to(dtype)\n",
    "# model = model.to(device)\n",
    "\n",
    "# logits, loss = model(x,y)\n",
    "# print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    print(\"TORCH :: Compiling Model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"Milion Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Loss Averages\n",
    "@torch.no_grad()\n",
    "def calculate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','eval']:\n",
    "        l = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x,y)\n",
    "            l[i] = loss\n",
    "        out[split] = l.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "print(calculate_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settting up the optimizer\n",
    "\n",
    "# so here we're creating a dictionary with keys as parameter name and values as a tensor with all the parameters andd collecting all those which require gradients\n",
    "p_dict = {p_name : p for p_name, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "# we do weight dacay as we're limiting the values of weight like any weight cannot be greater than a value, this will bring flexibility and prevent weight value to reach too high.\n",
    "\n",
    "# Now here we're seperating those parameters which would not require for weight decay such as biaas\n",
    "# so, we're seperating the parameters whose dim are greater than 2 will require weight decay and tensors with dim less than 2 will no require weight decay \n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >=2]\n",
    "no_weight_decay_p = [p for n,p in p_dict.items() if p.dim() < 2]\n",
    "\n",
    "# here we set weight decay value which require weight decay as weight_decay = 0.01 and which doesnt rquire it will set the weight decay value for them as 0.0\n",
    "optimizer_groups = [\n",
    "    {'params':weight_decay_p, 'weight_decay': weight_decay},\n",
    "    {'params':no_weight_decay_p, 'weight_decay':0.0}\n",
    "]\n",
    "\n",
    "# These betas values are for ADAMW optimizer and suggested by scientist for better performance\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9,0.99))\n",
    "\n",
    "# so this scheduler is for LR so as we get near to our less loss value in the graph we want our learning rate to get decrease with that that's why we use this scheduler and CosineAnnealingLR which will do the same for us.\n",
    "# eta_min refers to do take lr rate less than this, so lr will never go down to lr/10.\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float('inf') # track the best validation score value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading checkpoints\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    print(\"LLM Loading Checkpoint\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iteration = checkpoint['iteration']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration,loss\n",
    "\n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new sample\n",
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device) # Tokenize string -> (tensor of ids)\n",
    "    t1 = t1[None,:]  # (1 , [size of ids])\n",
    "    newgen = model.generate(t1,max=64)[0].tolist() # call the generate method, limit output size\n",
    "    result=decode(newgen) # decode the result with the tokenizer to get back characters\n",
    "    print(f\"{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "if inference:\n",
    "    model.eval()\n",
    "    while True:\n",
    "        qs = input(\"Enter you text here (q to quit): \")\n",
    "        if qs == \"\":\n",
    "            continue\n",
    "        if qs == \"q\":\n",
    "            break\n",
    "        generate_sample(qs)\n",
    "    #sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration,train_iters)):\n",
    "        xb, yb = get_batch('train')\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # Evaluation Loss\n",
    "        if ( i % eval_interval == 0 or i == train_iters - 1):\n",
    "            l = calculate_loss()\n",
    "            print(f\"\\n{i} train loss: {l['train']} / val loss: {l['eval']}\")\n",
    "            # generate_sample(\"once upon a time\")\n",
    "\n",
    "            if l['eval'] < best_val_loss:\n",
    "                best_val_loss = l['eval']\n",
    "                print(f\"[Checkpoint]: Saving with loss: {best_val_loss}\")\n",
    "                torch.save({\n",
    "                    'model_state_dic': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss' : best_val_loss,\n",
    "                    'iteration' : i,\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "            \n",
    "            if wandb_log:\n",
    "                wandb.log({\n",
    "                    \"loss/train\": l['train'],\n",
    "                    \"loss/eval\" : l['eval'],\n",
    "                    \"lr\" : scheduler.get_last_lr()[0],\n",
    "                },\n",
    "                step = i)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Traning Interupted... Cleaning up!!\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU Memmory\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released\")\n",
    "    #sys.exit(0)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
